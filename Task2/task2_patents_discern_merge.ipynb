{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task #2: Merging PatentsView, DISCERN, and Clinical Trials\n",
    "## Biopharma Firm's AI Capabilities via Patent Applications\n",
    "#### Edward Jung\n",
    "\n",
    "**Objective:** Construct a firm-year dataset of AI-related patent applications for firms conducting clinical trials.\n",
    "\n",
    "**Key Differences from Task #1:**\n",
    "- Focus on **patent applications** (not just granted patents)\n",
    "- Use **g_application** table (captures earliest innovation timing)\n",
    "- Map to **gvkey** using DISCERN 2 database\n",
    "- Time period: **2000-2025**\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Architecture Design](#1-data-architecture-design)\n",
    "2. [Data Import & Setup](#2-data-import--setup)\n",
    "3. [PatentsView: Patent Applications (2000-2025)](#3-patentsview-patent-applications-2000-2025)\n",
    "4. [DISCERN 2: Firm-to-GVKEY Mapping](#4-discern-2-firm-to-gvkey-mapping)\n",
    "5. [AI Classification Logic](#5-ai-classification-logic)\n",
    "6. [Firm-Year Aggregation](#6-firm-year-aggregation)\n",
    "7. [Merge with Clinical Trials Dataset](#7-merge-with-clinical-trials-dataset)\n",
    "8. [Export Final Datasets](#8-export-final-datasets)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Architecture Design\n",
    "\n",
    "### Recommended Two-Layer Approach\n",
    "\n",
    "#### Layer 1: **Patent-Level Dataset** (Intermediate)\n",
    "One row per patent application\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| application_id | str | Unique patent application identifier |\n",
    "| patent_id | str | Patent ID if granted (may be null) |\n",
    "| filing_date | date | Application filing date |\n",
    "| filing_year | int | Year extracted from filing_date |\n",
    "| assignee_name | str | Raw assignee/applicant name |\n",
    "| gvkey | str | GVKEY from DISCERN 2 mapping |\n",
    "| is_ai | bool | Binary: 1 if AI-related, 0 otherwise |\n",
    "| ai_method | str | How AI was identified: 'cpc', 'keyword', or 'both' |\n",
    "| ai_cpc_codes | str | Comma-separated AI CPC codes found |\n",
    "| ai_keywords | str | AI keywords matched in title/abstract |\n",
    "| title | str | Patent application title |\n",
    "| abstract | str | Patent abstract text |\n",
    "\n",
    "#### Layer 2: **Firm-Year Dataset** (Final Output)\n",
    "One row per gvkey-year combination\n",
    "\n",
    "| Column | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| gvkey | str | Firm identifier from Compustat |\n",
    "| year | int | Calendar year |\n",
    "| total_applications | int | Total patent applications filed |\n",
    "| ai_applications | int | AI-related applications |\n",
    "| ai_share | float | ai_applications / total_applications |\n",
    "| ai_dummy | int | 1 if ai_applications > 0, else 0 |\n",
    "\n",
    "### Memory Efficiency Strategy\n",
    "\n",
    "1. **Chunked Reading:** Process large TSV files in chunks (100k-500k rows)\n",
    "2. **Categorical Types:** Convert repetitive strings (year, gvkey) to category dtype\n",
    "3. **Early Filtering:** Filter to 2000-2025 and relevant firms before loading full data\n",
    "4. **DuckDB Pre-filtering:** Use SQL to filter before pandas import\n",
    "5. **Column Pruning:** Drop unnecessary columns immediately after import\n",
    "6. **Incremental Processing:** Process year-by-year if memory constrained\n",
    "\n",
    "### Why Two Layers?\n",
    "\n",
    "- **Patent-level:** Allows validation, spot-checking, manual review\n",
    "- **Firm-year:** Research-ready for regression analysis\n",
    "- **Reproducibility:** Can regenerate firm-year from patent-level if needed\n",
    "- **Flexibility:** Easy to add new metrics without re-processing raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Import & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n",
      "Working directory: /Users/eddiejung/Desktop/Research /Deliverables/Task2\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For large file processing\n",
    "import duckdb\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Clinical Trials Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinical Trials Dataset Shape: (9428, 8)\n",
      "\n",
      "Columns: ['nct_id', 'brief_title', 'overall_status', 'sponsor_name', 'gvkey_sponsor', 'phase_number', 'start_date', 'start_year']\n",
      "\n",
      "Date range: 2008 - 2021\n",
      "\n",
      "Unique firms (gvkey): 673\n",
      "Unique NCT IDs: 9428\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nct_id</th>\n",
       "      <th>brief_title</th>\n",
       "      <th>overall_status</th>\n",
       "      <th>sponsor_name</th>\n",
       "      <th>gvkey_sponsor</th>\n",
       "      <th>phase_number</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NCT00175851</td>\n",
       "      <td>Open Label Trial to Study the Long-term Safety...</td>\n",
       "      <td>Withdrawn</td>\n",
       "      <td>UCB Pharma</td>\n",
       "      <td>24454</td>\n",
       "      <td>3</td>\n",
       "      <td>2008-05-01</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NCT00359632</td>\n",
       "      <td>Study to Evaluate Eye Function in Patients Tak...</td>\n",
       "      <td>Terminated</td>\n",
       "      <td>Pfizer</td>\n",
       "      <td>8530</td>\n",
       "      <td>3</td>\n",
       "      <td>2008-11-01</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NCT00415155</td>\n",
       "      <td>A Study of LY2181308 in Patients With Advanced...</td>\n",
       "      <td>Withdrawn</td>\n",
       "      <td>Eli Lilly and Company</td>\n",
       "      <td>6730</td>\n",
       "      <td>2</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCT00422110</td>\n",
       "      <td>A Study to Evaluate the Efficacy and Safety of...</td>\n",
       "      <td>Withdrawn</td>\n",
       "      <td>UCB Pharma</td>\n",
       "      <td>24454</td>\n",
       "      <td>3</td>\n",
       "      <td>2008-05-01</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCT00422422</td>\n",
       "      <td>Open-label, Pharmacokinetic, Safety and Effica...</td>\n",
       "      <td>Completed</td>\n",
       "      <td>UCB Pharma</td>\n",
       "      <td>24454</td>\n",
       "      <td>2</td>\n",
       "      <td>2011-07-01</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nct_id                                        brief_title overall_status           sponsor_name  gvkey_sponsor  phase_number  start_date  start_year\n",
       "0  NCT00175851  Open Label Trial to Study the Long-term Safety...      Withdrawn             UCB Pharma          24454             3  2008-05-01        2008\n",
       "1  NCT00359632  Study to Evaluate Eye Function in Patients Tak...     Terminated                 Pfizer           8530             3  2008-11-01        2008\n",
       "2  NCT00415155  A Study of LY2181308 in Patients With Advanced...      Withdrawn  Eli Lilly and Company           6730             2  2008-08-01        2008\n",
       "3  NCT00422110  A Study to Evaluate the Efficacy and Safety of...      Withdrawn             UCB Pharma          24454             3  2008-05-01        2008\n",
       "4  NCT00422422  Open-label, Pharmacokinetic, Safety and Effica...      Completed             UCB Pharma          24454             2  2011-07-01        2011"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load clinical trials sample\n",
    "clinical_trials = pd.read_csv('clinical_trial_sample (1).csv')\n",
    "\n",
    "print(f\"Clinical Trials Dataset Shape: {clinical_trials.shape}\")\n",
    "print(f\"\\nColumns: {clinical_trials.columns.tolist()}\")\n",
    "print(f\"\\nDate range: {clinical_trials['start_year'].min()} - {clinical_trials['start_year'].max()}\")\n",
    "print(f\"\\nUnique firms (gvkey): {clinical_trials['gvkey_sponsor'].nunique()}\")\n",
    "print(f\"Unique NCT IDs: {clinical_trials['nct_id'].nunique()}\")\n",
    "\n",
    "# Display sample\n",
    "clinical_trials.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique GVKEYs: 673\n",
      "Number of unique sponsor names: 691\n",
      "\n",
      "Sample sponsor names:\n",
      "  - UCB Pharma\n",
      "  - Pfizer\n",
      "  - Eli Lilly and Company\n",
      "  - Amgen\n",
      "  - Mersana Therapeutics\n",
      "  - Organon and Co\n",
      "  - UCB Pharma SA\n",
      "  - GlaxoSmithKline\n",
      "  - Bayer\n",
      "  - Allergan\n"
     ]
    }
   ],
   "source": [
    "# Extract unique firms for filtering patent data\n",
    "unique_gvkeys = clinical_trials['gvkey_sponsor'].dropna().unique()\n",
    "unique_sponsors = clinical_trials['sponsor_name'].dropna().unique()\n",
    "\n",
    "print(f\"Number of unique GVKEYs: {len(unique_gvkeys)}\")\n",
    "print(f\"Number of unique sponsor names: {len(unique_sponsors)}\")\n",
    "print(f\"\\nSample sponsor names:\")\n",
    "for sponsor in unique_sponsors[:10]:\n",
    "    print(f\"  - {sponsor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PatentsView: Patent Applications (2000-2025)\n",
    "\n",
    "### Key PatentsView Tables for Applications\n",
    "\n",
    "According to Task #2 requirements, we need **application-level** data:\n",
    "\n",
    "1. **g_application** - Core application information\n",
    "   - `application_id`: Unique identifier\n",
    "   - `filing_date`: Application date (KEY for temporal alignment)\n",
    "   - `patent_id`: Patent ID if granted (may be NULL for pending)\n",
    "   \n",
    "2. **pg_applicant_not_disambiguated** - Applicant names (for DISCERN matching)\n",
    "   - `application_id`: Links to g_application\n",
    "   - `applicant_organization`: Company name (raw, not disambiguated)\n",
    "   \n",
    "3. **g_cpc_current** - CPC classification codes (for AI identification)\n",
    "   - Can link via `patent_id` (only for granted applications)\n",
    "   \n",
    "4. **g_us_application_citation** or **g_patent_abstract** - For keyword search\n",
    "\n",
    "### Data Download Strategy\n",
    "\n",
    "**Option A: Download from PatentsView bulk data**\n",
    "- Base URL: https://s3.amazonaws.com/data.patentsview.org/download/\n",
    "- Files: `g_application.tsv.zip`, `pg_applicant_not_disambiguated.tsv.zip`\n",
    "\n",
    "**Option B: Use existing Task1 data + supplement**\n",
    "- Task1 has granted patents (2021)\n",
    "- Need to download application-specific tables\n",
    "\n",
    "**Recommended: Option A** (complete application data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download utility loaded.\n"
     ]
    }
   ],
   "source": [
    "# Utility function for downloading PatentsView data\n",
    "def download_patentsview_table(table_name, data_dir='../Task1', overwrite=False):\n",
    "    \"\"\"\n",
    "    Download and extract PatentsView table.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    table_name : str\n",
    "        Name of table (e.g., 'g_application')\n",
    "    data_dir : str\n",
    "        Directory to save files\n",
    "    overwrite : bool\n",
    "        Whether to re-download if file exists\n",
    "    \"\"\"\n",
    "    base_url = \"https://s3.amazonaws.com/data.patentsview.org/download\"\n",
    "    zip_url = f\"{base_url}/{table_name}.tsv.zip\"\n",
    "    filename = f\"{table_name}.tsv\"\n",
    "    filepath = Path(data_dir) / filename\n",
    "    \n",
    "    # Create directory if doesn't exist\n",
    "    Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if filepath.exists() and not overwrite:\n",
    "        print(f\"✓ {filename} already exists\")\n",
    "        return str(filepath)\n",
    "    \n",
    "    print(f\"Downloading {table_name}...\")\n",
    "    zip_path = filepath.with_suffix('.tsv.zip')\n",
    "    \n",
    "    try:\n",
    "        # Download ZIP file\n",
    "        urlretrieve(zip_url, zip_path)\n",
    "        \n",
    "        # Extract TSV\n",
    "        with ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_dir)\n",
    "        \n",
    "        # Remove ZIP file\n",
    "        zip_path.unlink()\n",
    "        \n",
    "        print(f\"✓ Downloaded and extracted {filename}\")\n",
    "        return str(filepath)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {table_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"Download utility loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking/downloading required PatentsView tables...\n",
      "\n",
      "Downloading g_application...\n",
      "✓ Downloaded and extracted g_application.tsv\n",
      "Downloading pg_applicant_not_disambiguated...\n",
      "✗ Error downloading pg_applicant_not_disambiguated: HTTP Error 403: Forbidden\n",
      "✓ g_cpc_current.tsv already exists\n",
      "✓ g_patent_abstract.tsv already exists\n",
      "\n",
      "✓ All required tables ready.\n"
     ]
    }
   ],
   "source": [
    "# Download required tables (THIS MAY TAKE SEVERAL MINUTES)\n",
    "required_tables = [\n",
    "    'g_application',                    # Core application data\n",
    "    'pg_applicant_not_disambiguated',   # Applicant names for matching\n",
    "    'g_cpc_current',                    # Already downloaded in Task1\n",
    "    'g_patent_abstract'                 # Already downloaded in Task1\n",
    "]\n",
    "\n",
    "print(\"Checking/downloading required PatentsView tables...\\n\")\n",
    "for table in required_tables:\n",
    "    download_patentsview_table(table)\n",
    "    \n",
    "print(\"\\n✓ All required tables ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-Efficient Import Using DuckDB\n",
    "\n",
    "**Why DuckDB?**\n",
    "- Handles multi-GB files without loading into memory\n",
    "- SQL interface for filtering before pandas import\n",
    "- Fast aggregations and joins\n",
    "- No server setup required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB initialized: task2_patents.ddb\n",
      "Database location: /Users/eddiejung/Desktop/Research /Deliverables/Task2/task2_patents.ddb\n"
     ]
    }
   ],
   "source": [
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect('task2_patents.ddb')\n",
    "\n",
    "print(\"DuckDB initialized: task2_patents.ddb\")\n",
    "print(f\"Database location: {os.path.abspath('task2_patents.ddb')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing g_application (this may take 2-3 minutes)...\n",
      "✓ Total applications loaded: 9,359,185\n",
      "\n",
      "Sample records:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>application_id</th>\n",
       "      <th>patent_id</th>\n",
       "      <th>patent_application_type</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>series_code</th>\n",
       "      <th>rule_47_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05497504</td>\n",
       "      <td>3963197</td>\n",
       "      <td>05</td>\n",
       "      <td>1074-08-14</td>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05508062</td>\n",
       "      <td>3933359</td>\n",
       "      <td>05</td>\n",
       "      <td>1074-09-23</td>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05518254</td>\n",
       "      <td>3941467</td>\n",
       "      <td>05</td>\n",
       "      <td>1074-10-29</td>\n",
       "      <td>05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  application_id patent_id patent_application_type filing_date series_code rule_47_flag\n",
       "0       05497504   3963197                      05  1074-08-14          05            0\n",
       "1       05508062   3933359                      05  1074-09-23          05            0\n",
       "2       05518254   3941467                      05  1074-10-29          05            0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import g_application table\n",
    "print(\"Importing g_application (this may take 2-3 minutes)...\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE g_application AS \n",
    "    SELECT * FROM read_csv('../Task1/g_application.tsv', \n",
    "                           delim='\\t', \n",
    "                           header=true,\n",
    "                           all_varchar=true)\n",
    "\"\"\")\n",
    "\n",
    "# Check import\n",
    "result = con.execute(\"SELECT COUNT(*) as total FROM g_application\").fetchdf()\n",
    "print(f\"✓ Total applications loaded: {result['total'].iloc[0]:,}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample records:\")\n",
    "con.execute(\"SELECT * FROM g_application LIMIT 3\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to 2000-2025 application years...\n",
      "\n",
      "Applications by year (2000-2025):\n",
      "    filing_year  application_count\n",
      "0          2000             214632\n",
      "1          2001             228021\n",
      "2          2002             228953\n",
      "3          2003             220579\n",
      "4          2004             221454\n",
      "5          2005             226484\n",
      "6          2006             231706\n",
      "7          2007             240271\n",
      "8          2008             241976\n",
      "9          2009             233383\n",
      "10         2010             248378\n",
      "11         2011             268844\n",
      "12         2012             294133\n",
      "13         2013             313032\n",
      "14         2014             322929\n",
      "15         2015             335484\n",
      "16         2016             347454\n",
      "17         2017             362653\n",
      "18         2018             367908\n",
      "19         2019             388318\n",
      "20         2020             361312\n",
      "21         2021             318982\n",
      "22         2022             250221\n",
      "23         2023             155607\n",
      "24         2024              54392\n",
      "25         2025               4733\n",
      "\n",
      "Total applications (2000-2025): 6,681,839\n"
     ]
    }
   ],
   "source": [
    "# Filter to 2000-2025 applications\n",
    "print(\"Filtering to 2000-2025 application years...\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW applications_2000_2025 AS\n",
    "    SELECT *,\n",
    "           CAST(SUBSTRING(filing_date, 1, 4) AS INTEGER) as filing_year\n",
    "    FROM g_application\n",
    "    WHERE filing_date >= '2000-01-01' \n",
    "      AND filing_date <= '2025-12-31'\n",
    "\"\"\")\n",
    "\n",
    "# Count applications by year\n",
    "yearly_counts = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        filing_year,\n",
    "        COUNT(*) as application_count\n",
    "    FROM applications_2000_2025\n",
    "    GROUP BY filing_year\n",
    "    ORDER BY filing_year\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nApplications by year (2000-2025):\")\n",
    "print(yearly_counts)\n",
    "print(f\"\\nTotal applications (2000-2025): {yearly_counts['application_count'].sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing pg_applicant_not_disambiguated...\n"
     ]
    },
    {
     "ename": "IOException",
     "evalue": "IO Error: No files found that match the pattern \"../Task1/pg_applicant_not_disambiguated.tsv\"\n\nLINE 3:     SELECT * FROM read_csv('../Task1/pg_applicant_not_disambiguated.tsv', \n                          ^",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIOException\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import applicant names\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mImporting pg_applicant_not_disambiguated...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mcon\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43m    CREATE OR REPLACE TABLE pg_applicant AS \u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m    SELECT * FROM read_csv(\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../Task1/pg_applicant_not_disambiguated.tsv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m, \u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m                           delim=\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m, \u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m                           header=true,\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43m                           all_varchar=true)\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m result = con.execute(\u001b[33m\"\u001b[39m\u001b[33mSELECT COUNT(*) as total FROM pg_applicant\u001b[39m\u001b[33m\"\u001b[39m).fetchdf()\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Total applicant records: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m'\u001b[39m].iloc[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIOException\u001b[39m: IO Error: No files found that match the pattern \"../Task1/pg_applicant_not_disambiguated.tsv\"\n\nLINE 3:     SELECT * FROM read_csv('../Task1/pg_applicant_not_disambiguated.tsv', \n                          ^"
     ]
    }
   ],
   "source": [
    "# Import applicant names\n",
    "print(\"Importing pg_applicant_not_disambiguated...\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE TABLE pg_applicant AS \n",
    "    SELECT * FROM read_csv('../Task1/pg_applicant_not_disambiguated.tsv', \n",
    "                           delim='\\t', \n",
    "                           header=true,\n",
    "                           all_varchar=true)\n",
    "\"\"\")\n",
    "\n",
    "result = con.execute(\"SELECT COUNT(*) as total FROM pg_applicant\").fetchdf()\n",
    "print(f\"✓ Total applicant records: {result['total'].iloc[0]:,}\")\n",
    "\n",
    "# Show sample with organization names\n",
    "print(\"\\nSample applicant records:\")\n",
    "con.execute(\"\"\"\n",
    "    SELECT * FROM pg_applicant \n",
    "    WHERE applicant_organization IS NOT NULL \n",
    "    LIMIT 5\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DISCERN 2: Firm-to-GVKEY Mapping\n",
    "\n",
    "### DISCERN 2 Overview\n",
    "\n",
    "**Reference:** https://zenodo.org/records/13619821\n",
    "\n",
    "DISCERN 2 provides:\n",
    "- Mapping between patent assignees and Compustat GVKEY\n",
    "- Handles name variations and disambiguation\n",
    "- Time-varying firm identifiers\n",
    "\n",
    "### Required DISCERN 2 Files\n",
    "\n",
    "1. **Main mapping file:** Links assignee names → gvkey\n",
    "2. **Time-varying mappings:** Tracks firm changes over time (mergers, acquisitions)\n",
    "\n",
    "### Implementation Strategy\n",
    "\n",
    "**Option A: Direct Name Matching**\n",
    "- Match `pg_applicant.applicant_organization` to DISCERN assignee names\n",
    "- Join on cleaned/standardized names\n",
    "\n",
    "**Option B: Use Existing Clinical Trials Mapping**\n",
    "- Clinical trials dataset already has sponsor_name → gvkey mapping\n",
    "- Use this as ground truth for fuzzy matching to patent applicants\n",
    "\n",
    "**Recommended: Hybrid Approach**\n",
    "1. Start with clinical trials sponsor names\n",
    "2. Match to patent applicant names using fuzzy matching\n",
    "3. Validate with DISCERN 2 where available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder: DISCERN 2 Integration\n",
    "\n",
    "**Note:** DISCERN 2 data files are not included in this repository. \n",
    "\n",
    "**To integrate DISCERN 2:**\n",
    "1. Download from https://zenodo.org/records/13619821\n",
    "2. Extract relevant tables (consult data dictionary)\n",
    "3. Load into DuckDB or pandas\n",
    "4. Implement join logic below\n",
    "\n",
    "**For now, we'll use the clinical trials dataset's existing gvkey mapping as a proxy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sponsor name → gvkey lookup from clinical trials\n",
    "sponsor_gvkey_map = (\n",
    "    clinical_trials[['sponsor_name', 'gvkey_sponsor']]\n",
    "    .drop_duplicates()\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "print(f\"Sponsor-GVKEY mapping created: {len(sponsor_gvkey_map)} unique mappings\")\n",
    "print(f\"\\nSample mappings:\")\n",
    "sponsor_gvkey_map.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Clean organization names for matching\n",
    "def clean_org_name(name):\n",
    "    \"\"\"\n",
    "    Standardize organization names for matching.\n",
    "    \n",
    "    Removes:\n",
    "    - Legal suffixes (Inc., Corp., Ltd., etc.)\n",
    "    - Punctuation\n",
    "    - Extra whitespace\n",
    "    \n",
    "    Converts to lowercase.\n",
    "    \"\"\"\n",
    "    if pd.isna(name):\n",
    "        return ''\n",
    "    \n",
    "    name = str(name).lower()\n",
    "    \n",
    "    # Remove legal suffixes\n",
    "    suffixes = [\n",
    "        r'\\binc\\.?\\b', r'\\bcorp\\.?\\b', r'\\bcorporation\\b',\n",
    "        r'\\bltd\\.?\\b', r'\\blimited\\b', r'\\bco\\.?\\b',\n",
    "        r'\\bllc\\b', r'\\blp\\b', r'\\bplc\\b',\n",
    "        r'\\bsa\\b', r'\\bag\\b', r'\\bgmbh\\b'\n",
    "    ]\n",
    "    for suffix in suffixes:\n",
    "        name = re.sub(suffix, '', name)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    name = re.sub(r'[^a-z0-9\\s]', ' ', name)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    name = ' '.join(name.split())\n",
    "    \n",
    "    return name.strip()\n",
    "\n",
    "# Test cleaning function\n",
    "test_names = [\n",
    "    'Pfizer Inc.',\n",
    "    'Bristol-Myers Squibb',\n",
    "    'Eli Lilly and Company',\n",
    "    'Novartis AG'\n",
    "]\n",
    "\n",
    "print(\"Name cleaning examples:\")\n",
    "for name in test_names:\n",
    "    print(f\"  {name:30s} → {clean_org_name(name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaned name lookup\n",
    "sponsor_gvkey_map['sponsor_name_clean'] = sponsor_gvkey_map['sponsor_name'].apply(clean_org_name)\n",
    "\n",
    "# Remove duplicates after cleaning (some names may collapse to same cleaned version)\n",
    "sponsor_lookup = (\n",
    "    sponsor_gvkey_map\n",
    "    .groupby('sponsor_name_clean')['gvkey_sponsor']\n",
    "    .first()  # Take first gvkey if multiple matches\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "print(f\"Cleaned sponsor lookup created: {len(sponsor_lookup)} unique clean names\")\n",
    "print(f\"\\nSample lookup:\")\n",
    "for i, (clean_name, gvkey) in enumerate(list(sponsor_lookup.items())[:10]):\n",
    "    print(f\"  {clean_name:30s} → {gvkey}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Patent Applicants to GVKEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract applicants for applications in our time window\n",
    "print(\"Extracting applicants for 2000-2025 applications...\")\n",
    "\n",
    "applicants_df = con.execute(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        a.application_id,\n",
    "        a.filing_date,\n",
    "        a.filing_year,\n",
    "        a.patent_id,\n",
    "        p.applicant_organization\n",
    "    FROM applications_2000_2025 a\n",
    "    INNER JOIN pg_applicant p ON a.application_id = p.application_id\n",
    "    WHERE p.applicant_organization IS NOT NULL\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Extracted {len(applicants_df):,} application-applicant pairs\")\n",
    "print(f\"\\nSample:\")\n",
    "applicants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean applicant names and match to gvkey\n",
    "print(\"Matching applicant names to GVKEY...\")\n",
    "\n",
    "applicants_df['applicant_clean'] = applicants_df['applicant_organization'].apply(clean_org_name)\n",
    "applicants_df['gvkey'] = applicants_df['applicant_clean'].map(sponsor_lookup)\n",
    "\n",
    "# Check match rate\n",
    "match_rate = (applicants_df['gvkey'].notna().sum() / len(applicants_df)) * 100\n",
    "print(f\"\\nMatch rate: {match_rate:.2f}%\")\n",
    "print(f\"Matched applications: {applicants_df['gvkey'].notna().sum():,}\")\n",
    "print(f\"Unmatched applications: {applicants_df['gvkey'].isna().sum():,}\")\n",
    "\n",
    "# Filter to matched applications only\n",
    "matched_applications = applicants_df[applicants_df['gvkey'].notna()].copy()\n",
    "print(f\"\\n✓ Working with {len(matched_applications):,} matched applications\")\n",
    "\n",
    "matched_applications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check coverage: which firms from clinical trials have patent applications?\n",
    "print(\"Firms with patent applications:\")\n",
    "\n",
    "firms_with_patents = matched_applications['gvkey'].unique()\n",
    "firms_in_trials = clinical_trials['gvkey_sponsor'].dropna().unique()\n",
    "\n",
    "coverage = (len(set(firms_with_patents) & set(firms_in_trials)) / len(firms_in_trials)) * 100\n",
    "\n",
    "print(f\"  Clinical trial firms: {len(firms_in_trials)}\")\n",
    "print(f\"  Firms with patents: {len(firms_with_patents)}\")\n",
    "print(f\"  Overlap: {len(set(firms_with_patents) & set(firms_in_trials))}\")\n",
    "print(f\"  Coverage: {coverage:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AI Classification Logic\n",
    "\n",
    "### Two-Pronged AI Identification Strategy\n",
    "\n",
    "#### Method 1: CPC Classification Codes\n",
    "**High Precision Approach**\n",
    "\n",
    "AI-related CPC codes (from WIPO/EPO standards):\n",
    "- **G06N** - Computing based on specific computational models\n",
    "  - G06N3 - Neural networks\n",
    "  - G06N5 - Knowledge-based models\n",
    "  - G06N7 - Probabilistic/fuzzy logic\n",
    "  - G06N10 - Quantum computing\n",
    "  - G06N20 - Machine learning\n",
    "\n",
    "**Advantages:**\n",
    "- Examiner-assigned (authoritative)\n",
    "- Standardized internationally\n",
    "- High precision\n",
    "\n",
    "**Limitations:**\n",
    "- Only available for granted patents (not pending applications)\n",
    "- May miss emerging AI applications not yet classified\n",
    "\n",
    "#### Method 2: Keyword-Based Filtering\n",
    "**High Recall Approach**\n",
    "\n",
    "Search title/abstract for AI-related terms:\n",
    "- Core ML: machine learning, deep learning, neural network, artificial intelligence\n",
    "- Techniques: reinforcement learning, supervised learning, unsupervised learning\n",
    "- Models: random forest, gradient boosting, support vector machine\n",
    "- Applications: computer vision, natural language processing, NLP\n",
    "\n",
    "**Advantages:**\n",
    "- Works for both granted and pending applications\n",
    "- Captures emerging terminology\n",
    "- Higher recall\n",
    "\n",
    "**Limitations:**\n",
    "- May include false positives\n",
    "- Requires careful keyword curation\n",
    "\n",
    "### Recommended: Hybrid Approach\n",
    "- **Primary:** CPC codes (for granted patents)\n",
    "- **Secondary:** Keywords (for all applications, especially pending)\n",
    "- **Combined:** Mark as AI if either method flags it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AI-related CPC code patterns\n",
    "AI_CPC_PATTERNS = [\n",
    "    'G06N3',   # Neural networks\n",
    "    'G06N5',   # Knowledge-based models\n",
    "    'G06N7',   # Probabilistic/fuzzy logic\n",
    "    'G06N10',  # Quantum computing\n",
    "    'G06N20',  # Machine learning\n",
    "]\n",
    "\n",
    "# Define AI-related keywords (lowercase)\n",
    "AI_KEYWORDS = [\n",
    "    # Core ML terms\n",
    "    'machine learning', 'deep learning', 'neural network', 'artificial intelligence',\n",
    "    'ai model', 'ml model', 'deep neural', 'convolutional neural',\n",
    "    \n",
    "    # Learning paradigms\n",
    "    'supervised learning', 'unsupervised learning', 'reinforcement learning',\n",
    "    'semi-supervised', 'transfer learning', 'meta-learning',\n",
    "    \n",
    "    # Specific models\n",
    "    'random forest', 'gradient boosting', 'support vector machine', 'svm',\n",
    "    'decision tree', 'bayesian network', 'recurrent neural', 'lstm',\n",
    "    'transformer', 'attention mechanism', 'generative adversarial',\n",
    "    \n",
    "    # Applications\n",
    "    'computer vision', 'natural language processing', 'nlp',\n",
    "    'image recognition', 'speech recognition', 'predictive model',\n",
    "    \n",
    "    # Techniques\n",
    "    'feature extraction', 'dimensionality reduction', 'classification algorithm',\n",
    "    'regression algorithm', 'clustering algorithm'\n",
    "]\n",
    "\n",
    "print(f\"AI CPC patterns defined: {len(AI_CPC_PATTERNS)}\")\n",
    "print(f\"AI keywords defined: {len(AI_KEYWORDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Check if text contains AI keywords\n",
    "def contains_ai_keywords(text):\n",
    "    \"\"\"\n",
    "    Check if text contains any AI-related keywords.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (bool, list of matched keywords)\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return False, []\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    matched_keywords = []\n",
    "    \n",
    "    for keyword in AI_KEYWORDS:\n",
    "        if keyword in text_lower:\n",
    "            matched_keywords.append(keyword)\n",
    "    \n",
    "    return len(matched_keywords) > 0, matched_keywords\n",
    "\n",
    "# Test function\n",
    "test_texts = [\n",
    "    \"A machine learning approach to drug discovery\",\n",
    "    \"Novel pharmaceutical composition\",\n",
    "    \"Deep neural network for protein folding prediction\"\n",
    "]\n",
    "\n",
    "print(\"Keyword detection test:\")\n",
    "for text in test_texts:\n",
    "    is_ai, keywords = contains_ai_keywords(text)\n",
    "    print(f\"  {text[:50]:50s} → AI: {is_ai}, Keywords: {keywords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify Applications as AI-Related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get CPC codes for granted patents\n",
    "print(\"Extracting CPC codes for matched applications...\")\n",
    "\n",
    "# Get patent IDs from matched applications (only granted ones have CPC codes)\n",
    "granted_patent_ids = matched_applications['patent_id'].dropna().unique()\n",
    "\n",
    "print(f\"Granted patents in matched set: {len(granted_patent_ids):,}\")\n",
    "\n",
    "# Query CPC codes (using Task1 data)\n",
    "cpc_codes_df = con.execute(f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        patent_id,\n",
    "        cpc_group\n",
    "    FROM g_cpc_current\n",
    "    WHERE patent_id IN ({','.join(\"'\" + str(pid) + \"'\" for pid in granted_patent_ids[:1000])})\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Extracted CPC codes for {cpc_codes_df['patent_id'].nunique():,} patents\")\n",
    "cpc_codes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify AI patents based on CPC codes\n",
    "def is_ai_cpc(cpc_group):\n",
    "    \"\"\"Check if CPC code matches AI patterns.\"\"\"\n",
    "    if pd.isna(cpc_group):\n",
    "        return False\n",
    "    for pattern in AI_CPC_PATTERNS:\n",
    "        if str(cpc_group).startswith(pattern):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "cpc_codes_df['is_ai_cpc'] = cpc_codes_df['cpc_group'].apply(is_ai_cpc)\n",
    "\n",
    "# Get AI patents by CPC\n",
    "ai_patents_cpc = cpc_codes_df[cpc_codes_df['is_ai_cpc']]['patent_id'].unique()\n",
    "print(f\"AI patents identified by CPC: {len(ai_patents_cpc):,}\")\n",
    "\n",
    "# Get AI CPC codes found\n",
    "ai_cpc_codes = (\n",
    "    cpc_codes_df[cpc_codes_df['is_ai_cpc']]\n",
    "    .groupby('patent_id')['cpc_group']\n",
    "    .apply(lambda x: ','.join(x))\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get titles/abstracts for keyword search\n",
    "print(\"Loading patent titles and abstracts...\")\n",
    "\n",
    "# Get titles from g_application (if available) or g_patent\n",
    "# For now, using Task1 g_patent table as proxy\n",
    "titles_abstracts = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        p.patent_id,\n",
    "        p.patent_title as title,\n",
    "        a.patent_abstract as abstract\n",
    "    FROM g_patent p\n",
    "    LEFT JOIN g_patent_abstract a ON p.patent_id = a.patent_id\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"✓ Loaded titles/abstracts for {len(titles_abstracts):,} patents\")\n",
    "titles_abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply keyword detection\n",
    "print(\"Detecting AI keywords in titles and abstracts...\")\n",
    "\n",
    "# Combine title and abstract for search\n",
    "titles_abstracts['combined_text'] = (\n",
    "    titles_abstracts['title'].fillna('') + ' ' + \n",
    "    titles_abstracts['abstract'].fillna('')\n",
    ")\n",
    "\n",
    "# Apply keyword detection (this may take a few minutes for large datasets)\n",
    "keyword_results = titles_abstracts['combined_text'].apply(contains_ai_keywords)\n",
    "titles_abstracts['is_ai_keyword'] = keyword_results.apply(lambda x: x[0])\n",
    "titles_abstracts['ai_keywords_found'] = keyword_results.apply(lambda x: ','.join(x[1]))\n",
    "\n",
    "ai_patents_keyword = titles_abstracts[titles_abstracts['is_ai_keyword']]['patent_id'].unique()\n",
    "print(f\"\\n✓ AI patents identified by keywords: {len(ai_patents_keyword):,}\")\n",
    "\n",
    "# Show sample AI patents\n",
    "print(\"\\nSample AI patents identified by keywords:\")\n",
    "titles_abstracts[titles_abstracts['is_ai_keyword']][['patent_id', 'title', 'ai_keywords_found']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CPC and keyword classifications\n",
    "print(\"Combining CPC and keyword classifications...\")\n",
    "\n",
    "# Merge back to matched_applications\n",
    "matched_applications['is_ai_cpc'] = matched_applications['patent_id'].isin(ai_patents_cpc)\n",
    "matched_applications = matched_applications.merge(\n",
    "    titles_abstracts[['patent_id', 'is_ai_keyword', 'ai_keywords_found', 'title']],\n",
    "    on='patent_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create combined AI flag\n",
    "matched_applications['is_ai'] = (\n",
    "    matched_applications['is_ai_cpc'] | \n",
    "    matched_applications['is_ai_keyword'].fillna(False)\n",
    ")\n",
    "\n",
    "# Add AI method indicator\n",
    "def get_ai_method(row):\n",
    "    if row['is_ai_cpc'] and row['is_ai_keyword']:\n",
    "        return 'both'\n",
    "    elif row['is_ai_cpc']:\n",
    "        return 'cpc'\n",
    "    elif row['is_ai_keyword']:\n",
    "        return 'keyword'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "matched_applications['ai_method'] = matched_applications.apply(get_ai_method, axis=1)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n=== AI CLASSIFICATION SUMMARY ===\")\n",
    "print(f\"Total applications: {len(matched_applications):,}\")\n",
    "print(f\"AI by CPC only: {(matched_applications['ai_method'] == 'cpc').sum():,}\")\n",
    "print(f\"AI by keyword only: {(matched_applications['ai_method'] == 'keyword').sum():,}\")\n",
    "print(f\"AI by both methods: {(matched_applications['ai_method'] == 'both').sum():,}\")\n",
    "print(f\"Total AI applications: {matched_applications['is_ai'].sum():,}\")\n",
    "print(f\"AI share: {(matched_applications['is_ai'].sum() / len(matched_applications) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Firm-Year Aggregation\n",
    "\n",
    "### Create Research-Ready Firm-Year Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate to firm-year level\n",
    "print(\"Aggregating to firm-year level...\")\n",
    "\n",
    "firm_year = (\n",
    "    matched_applications\n",
    "    .groupby(['gvkey', 'filing_year'])\n",
    "    .agg({\n",
    "        'application_id': 'count',              # Total applications\n",
    "        'is_ai': 'sum'                          # AI applications\n",
    "    })\n",
    "    .rename(columns={\n",
    "        'application_id': 'total_applications',\n",
    "        'is_ai': 'ai_applications'\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate derived metrics\n",
    "firm_year['ai_share'] = firm_year['ai_applications'] / firm_year['total_applications']\n",
    "firm_year['ai_dummy'] = (firm_year['ai_applications'] > 0).astype(int)\n",
    "\n",
    "# Rename filing_year to year for clarity\n",
    "firm_year = firm_year.rename(columns={'filing_year': 'year'})\n",
    "\n",
    "print(f\"✓ Firm-year dataset created: {len(firm_year):,} observations\")\n",
    "print(f\"  Unique firms: {firm_year['gvkey'].nunique()}\")\n",
    "print(f\"  Year range: {firm_year['year'].min()} - {firm_year['year'].max()}\")\n",
    "\n",
    "firm_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== FIRM-YEAR DATASET SUMMARY ===\")\n",
    "print(f\"\\nSample size: {len(firm_year):,} firm-year observations\")\n",
    "print(f\"\\nApplications per firm-year:\")\n",
    "print(firm_year['total_applications'].describe())\n",
    "print(f\"\\nAI applications per firm-year:\")\n",
    "print(firm_year['ai_applications'].describe())\n",
    "print(f\"\\nAI share distribution:\")\n",
    "print(firm_year['ai_share'].describe())\n",
    "print(f\"\\nFirm-years with at least one AI patent: {firm_year['ai_dummy'].sum():,} ({firm_year['ai_dummy'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal trends: AI adoption over time\n",
    "yearly_trends = (\n",
    "    firm_year\n",
    "    .groupby('year')\n",
    "    .agg({\n",
    "        'total_applications': 'sum',\n",
    "        'ai_applications': 'sum',\n",
    "        'ai_dummy': 'sum',  # Number of firms with AI\n",
    "        'gvkey': 'nunique'  # Number of firms\n",
    "    })\n",
    "    .rename(columns={'gvkey': 'num_firms', 'ai_dummy': 'firms_with_ai'})\n",
    ")\n",
    "\n",
    "yearly_trends['ai_share'] = yearly_trends['ai_applications'] / yearly_trends['total_applications']\n",
    "yearly_trends['firm_adoption_rate'] = yearly_trends['firms_with_ai'] / yearly_trends['num_firms']\n",
    "\n",
    "print(\"\\n=== TEMPORAL TRENDS ===\")\n",
    "print(yearly_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Merge with Clinical Trials Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clinical trials firm-year dataset\n",
    "print(\"Creating clinical trials firm-year dataset...\")\n",
    "\n",
    "trials_firm_year = (\n",
    "    clinical_trials\n",
    "    .groupby(['gvkey_sponsor', 'start_year'])\n",
    "    .agg({\n",
    "        'nct_id': 'count',\n",
    "        'phase_number': 'mean'  # Average phase\n",
    "    })\n",
    "    .rename(columns={\n",
    "        'nct_id': 'num_trials',\n",
    "        'phase_number': 'avg_phase'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .rename(columns={'gvkey_sponsor': 'gvkey', 'start_year': 'year'})\n",
    ")\n",
    "\n",
    "print(f\"✓ Clinical trials firm-year: {len(trials_firm_year):,} observations\")\n",
    "trials_firm_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge patent and trial datasets\n",
    "print(\"Merging patent applications with clinical trials...\")\n",
    "\n",
    "merged_firm_year = firm_year.merge(\n",
    "    trials_firm_year,\n",
    "    on=['gvkey', 'year'],\n",
    "    how='outer',  # Keep all firm-years from both datasets\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "# Fill NAs with 0 for count variables\n",
    "count_vars = ['total_applications', 'ai_applications', 'ai_dummy', 'num_trials']\n",
    "for var in count_vars:\n",
    "    merged_firm_year[var] = merged_firm_year[var].fillna(0).astype(int)\n",
    "\n",
    "# Recalculate ai_share\n",
    "merged_firm_year['ai_share'] = np.where(\n",
    "    merged_firm_year['total_applications'] > 0,\n",
    "    merged_firm_year['ai_applications'] / merged_firm_year['total_applications'],\n",
    "    0\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Merged dataset: {len(merged_firm_year):,} firm-year observations\")\n",
    "print(f\"\\nMerge statistics:\")\n",
    "print(merged_firm_year['_merge'].value_counts())\n",
    "\n",
    "# Drop merge indicator\n",
    "merged_firm_year = merged_firm_year.drop('_merge', axis=1)\n",
    "\n",
    "merged_firm_year.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of merged dataset\n",
    "print(\"=== MERGED FIRM-YEAR DATASET SUMMARY ===\")\n",
    "print(f\"\\nTotal observations: {len(merged_firm_year):,}\")\n",
    "print(f\"Unique firms: {merged_firm_year['gvkey'].nunique()}\")\n",
    "print(f\"Year range: {merged_firm_year['year'].min()} - {merged_firm_year['year'].max()}\")\n",
    "print(f\"\\nFirm-years with patents: {(merged_firm_year['total_applications'] > 0).sum():,}\")\n",
    "print(f\"Firm-years with AI patents: {(merged_firm_year['ai_applications'] > 0).sum():,}\")\n",
    "print(f\"Firm-years with trials: {(merged_firm_year['num_trials'] > 0).sum():,}\")\n",
    "print(f\"Firm-years with both patents and trials: {((merged_firm_year['total_applications'] > 0) & (merged_firm_year['num_trials'] > 0)).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export patent-level dataset\n",
    "patent_level_output = matched_applications[[\n",
    "    'application_id', 'patent_id', 'filing_date', 'filing_year',\n",
    "    'gvkey', 'applicant_organization', 'applicant_clean',\n",
    "    'is_ai', 'ai_method', 'is_ai_cpc', 'is_ai_keyword', 'ai_keywords_found',\n",
    "    'title'\n",
    "]].copy()\n",
    "\n",
    "patent_level_output.to_csv('patent_level_dataset.csv', index=False)\n",
    "print(f\"✓ Exported patent-level dataset: patent_level_dataset.csv\")\n",
    "print(f\"  Shape: {patent_level_output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export firm-year dataset (patents only)\n",
    "firm_year.to_csv('firm_year_patents.csv', index=False)\n",
    "print(f\"✓ Exported firm-year patent dataset: firm_year_patents.csv\")\n",
    "print(f\"  Shape: {firm_year.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export merged firm-year dataset (patents + trials)\n",
    "merged_firm_year.to_csv('firm_year_merged.csv', index=False)\n",
    "print(f\"✓ Exported merged firm-year dataset: firm_year_merged.csv\")\n",
    "print(f\"  Shape: {merged_firm_year.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Deliverables Created\n",
    "\n",
    "1. **patent_level_dataset.csv**\n",
    "   - One row per patent application\n",
    "   - Contains AI classification flags and methods\n",
    "   - ~{patent_level_output.shape[0]:,} applications\n",
    "\n",
    "2. **firm_year_patents.csv**\n",
    "   - One row per gvkey-year\n",
    "   - Patent application metrics: total, AI count, AI share\n",
    "   - ~{firm_year.shape[0]:,} firm-year observations\n",
    "\n",
    "3. **firm_year_merged.csv**\n",
    "   - Combined patent applications + clinical trials\n",
    "   - Ready for regression analysis\n",
    "   - ~{merged_firm_year.shape[0]:,} firm-year observations\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- **Match Rate:** {match_rate:.1f}% of patent applications matched to clinical trial firms\n",
    "- **AI Patents:** {matched_applications['is_ai'].sum():,} AI-related applications identified\n",
    "- **AI Share:** {(matched_applications['is_ai'].mean()*100):.2f}% of applications are AI-related\n",
    "- **Temporal Coverage:** 2000-2025\n",
    "\n",
    "### Recommended Next Steps\n",
    "\n",
    "1. **DISCERN 2 Integration**\n",
    "   - Download DISCERN 2 database\n",
    "   - Improve gvkey matching coverage\n",
    "   - Handle time-varying firm identifiers (M&A)\n",
    "\n",
    "2. **Validation**\n",
    "   - Manual review of AI classification accuracy\n",
    "   - Compare to known AI patents/firms\n",
    "   - Refine keyword list based on false positives\n",
    "\n",
    "3. **Extended Analysis**\n",
    "   - Lag structures (patents → trials)\n",
    "   - Firm-specific AI intensity trends\n",
    "   - Technology subfield analysis (drug discovery vs. clinical trial AI)\n",
    "\n",
    "4. **PubMed Linkage** (Task #2 Part 2)\n",
    "   - Implement NCT ID → PubMed search\n",
    "   - Identify AI methods in trial publications\n",
    "\n",
    "### Memory Efficiency Notes\n",
    "\n",
    "**For larger datasets:**\n",
    "1. Process year-by-year in chunks\n",
    "2. Use DuckDB for all filtering/aggregation\n",
    "3. Keep only necessary columns in memory\n",
    "4. Use categorical dtypes for string columns\n",
    "5. Consider Dask/Vaex for very large datasets (>50GB)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
